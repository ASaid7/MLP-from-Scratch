{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Data loader & Preprocessing Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(os.path.expanduser('~'), 'Documents', 'OR 610')\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims = struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "        return np.fromstring(f.read(), dtype=np.uint8).reshape(shape)\n",
    "    \n",
    "def oneHotEncoding(label):\n",
    "    n = np.max(label)+1\n",
    "    v = np.eye(n)[label]\n",
    "    return v.T\n",
    "\n",
    "\n",
    "def imageProcess(data):\n",
    "    data = data/255\n",
    "    data = data.reshape(data.shape[0],data.shape[1]*data.shape[2])\n",
    "    return data.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define activation functions for forward pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softMax(X):\n",
    "    e = np.exp(X)\n",
    "    p = e/np.sum(e, axis=0)\n",
    "    return p\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0,z)\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1./(1.+np.exp(-z))\n",
    "\n",
    "\n",
    "def tanh(z):\n",
    "    return np.tanh(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define Activation functions for backward pass i.e. first derivative of the forward pass activation function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dReLU(z):\n",
    "    return (z > 0) * 1\n",
    "\n",
    "def dSigmoid(z):\n",
    "    return sigmoid(z) *(1-sigmoid (z))\n",
    "\n",
    "def dTanh(z):\n",
    "    return 1/(np.cosh(z)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi label cross entropy with L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Procedures**\n",
    "\n",
    "*Forward Pass:*\n",
    "\n",
    "\\\\(Z_i = W_i \\bullet x^T + b_i \\\\)\n",
    "\n",
    "\\\\(A_i = \\sigma(Z_i)\\\\)\n",
    "\n",
    "\\\\(\\hat{y} = A_i\\\\)\n",
    "\n",
    "where \\\\(\\sigma\\\\) is a nonlinear transformation\n",
    "\n",
    "*Loss Function* with regularization\n",
    "\n",
    "\\\\[L(y,\\hat{y}) = -\\frac{1}{m} \\Sigma_j \\Sigma_i y_i log(\\hat{y_i}) + \\frac{\\lambda}{2*m} * (\n",
    "\\Sigma_w w^2)\\\\]\n",
    "\n",
    "*Back propagation: here we use differental equations and use the chain rule first starting with the cost function and work backwards until we get to weights since we want to learn the weights that give a better fit*\n",
    "\n",
    "\\\\[\\frac{\\delta L}{\\delta w_i} = \\frac{\\delta L}{\\delta \\hat{y}} * \\frac{\\delta \\hat{y}}{\\delta z} * \\frac{\\delta z}{\\delta w_i}\\\\]\n",
    "\n",
    "*Update weights*\n",
    "\n",
    "\\\\[w_i = w_i - \\eta * \\delta w_i - \\frac {(w_i * \\lambda * \\eta)}{m}\\\\]\n",
    "\n",
    "where \\\\(\\eta\\\\) is the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyR2(y, y_hat, lamda, params):\n",
    "    m = y.shape[1]\n",
    "    cost = -(1/m) * np.sum(y*np.log(y_hat)) + lamda/(2*m) * (np.sum(params['W1']**2) + np.sum(params['W2']**2))\n",
    "    return cost\n",
    "\n",
    "def forward(X,params,activation):\n",
    "\n",
    "    forwardPass = {}\n",
    "    forwardPass['Z1'] = np.matmul(params['W1'], X) + params['b1']\n",
    "    forwardPass['A1'] = activation(forwardPass['Z1'])\n",
    "    forwardPass['Z2'] = np.matmul(params['W2'],forwardPass['A1']) + params['b2']\n",
    "    forwardPass['A2'] = softMax(forwardPass['Z2'])\n",
    "    return forwardPass\n",
    "\n",
    "\n",
    "def back(X, y,forwardPass, params,dActivation):\n",
    "    m = X.shape[1]\n",
    "    gradient = {}\n",
    "    gradient['dZ2'] = forwardPass['A2'] - y\n",
    "    gradient['dW2'] = (1./m) * np.matmul(gradient['dZ2'], forwardPass['A1'].T)\n",
    "    gradient['db2'] = (1./m) * np.sum(gradient['dZ2'], axis=1, keepdims=True)\n",
    "    gradient['dA1'] = np.matmul(params['W2'].T, gradient['dZ2'])\n",
    "    gradient['dZ1'] = gradient['dA1'] * dActivation(forwardPass['Z1'])\n",
    "    gradient['dW1'] = (1./m) * np.matmul(gradient['dZ1'], X.T)\n",
    "    gradient['db1'] = (1./m) * np.sum(gradient['dZ1'])\n",
    "    return gradient\n",
    "\n",
    "def updater(params,grad,eta,lamda,m):\n",
    "    updatedParams = {}\n",
    "    updatedParams['W2'] = params['W2'] - eta * grad['dW2'] - (params['W2']*lamda*eta)/m\n",
    "    updatedParams['b2'] = params['b2'] - eta * grad['db2']\n",
    "    updatedParams['W1'] = params['W1'] - eta * grad['dW1'] - (params['W1']*lamda*eta)/m\n",
    "    updatedParams['b1'] = params['b1'] - eta * grad['db1']\n",
    "    return updatedParams\n",
    "\n",
    "def classifer(X, params,activation):\n",
    "    Z1 = np.matmul(params['W1'], X) + params['b1']\n",
    "    A1 = activation(Z1)\n",
    "    Z2 = np.matmul(params['W2'],A1) + params['b2']\n",
    "    A2 = softMax(Z2)\n",
    "    pred = np.argmax(A2, axis=0)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data to memory and define hyper params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdullah/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "X_train = imageProcess(read_idx(path+'/train-images.idx3-ubyte'))\n",
    "y_train = oneHotEncoding(read_idx(path+'/train-labels-idx1-ubyte'))\n",
    "X_test = imageProcess(read_idx(path+'/t10k-images-idx3-ubyte'))\n",
    "y_test = read_idx(path+'/t10k-labels-idx1-ubyte')\n",
    "\n",
    "#### General Hyperparameters\n",
    "m=10000 #batch size\n",
    "n_x = X_train.shape[0]\n",
    "n_h = 100\n",
    "eta = 1\n",
    "lamda = 2\n",
    "np.random.seed(7)\n",
    "epoch = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid - Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost: 0.2308652789490603\n",
      "time to train: 0:00:51.425122\n",
      "Accuracy: 0.9478\n"
     ]
    }
   ],
   "source": [
    "#m = X_train.shape[1]\n",
    "#Initializing weightss\n",
    "sigmoidParams = {'W1': np.random.randn(n_h, n_x)* np.sqrt(1. / n_x),\n",
    "                 'b1': np.zeros((n_h, 1)),\n",
    "                 'W2': np.random.randn(10, n_h)* np.sqrt(1. / n_h),\n",
    "                 'b2': np.zeros((10, 1))\n",
    "                 }\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(epoch):\n",
    "    #shuffle batch index\n",
    "    idx = np.random.permutation(X_train.shape[1])[:m]\n",
    "    X=X_train[:,idx]\n",
    "    y=y_train[:,idx]\n",
    "    #forward pass\n",
    "    forwardPass = forward(X,sigmoidParams,sigmoid)\n",
    "    #cost\n",
    "    cost = crossEntropyR2(y, forwardPass['A2'], lamda, sigmoidParams)\n",
    "    #back Prop\n",
    "    gradient = back(X, y, forwardPass, sigmoidParams,dSigmoid)\n",
    "    #updating weights\n",
    "    sigmoidParams=updater(sigmoidParams,gradient,eta,lamda,m)\n",
    "difference = datetime.now() - start\n",
    "print(\"Final cost:\", cost)\n",
    "print('time to train:', difference)\n",
    "\n",
    "y_hat = classifer(X_test, sigmoidParams, sigmoid)\n",
    "\n",
    "\n",
    "print('Accuracy:',sum(y_hat==y_test)*1/len(y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost: 0.10742655805231126\n",
      "time to train: 0:00:40.840828\n",
      "Accuracy: 0.9721\n"
     ]
    }
   ],
   "source": [
    "#######RELU SECTION ############\n",
    "reluParams = {'W1': np.random.randn(n_h, n_x)* np.sqrt(2. / n_x),\n",
    "                 'b1': np.zeros((n_h, 1)),\n",
    "                 'W2': np.random.randn(10, n_h)* np.sqrt(2. / n_h),\n",
    "                 'b2': np.zeros((10, 1))\n",
    "                 }\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(epoch):\n",
    "    #shuffle batch index\n",
    "    idx = np.random.permutation(X_train.shape[1])[:m]\n",
    "    X=X_train[:,idx]\n",
    "    y=y_train[:,idx]\n",
    "    #forward pass\n",
    "    forwardPass = forward(X,reluParams,ReLU)\n",
    "    #cost\n",
    "    cost = crossEntropyR2(y, forwardPass['A2'], lamda, reluParams)\n",
    "    #back Prop\n",
    "    gradient = back(X, y, forwardPass, reluParams,dReLU)\n",
    "    #updating weights\n",
    "    reluParams=updater(reluParams,gradient,eta,lamda,m)\n",
    "difference = datetime.now() - start\n",
    "print(\"Final cost:\", cost)\n",
    "print('time to train:', difference)\n",
    "\n",
    "y_hat = classifer(X_test, reluParams, ReLU)\n",
    "\n",
    "\n",
    "print('Accuracy:',sum(y_hat==y_test)*1/len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost: 0.12754923340771634\n",
      "time to train: 0:00:39.966550\n",
      "Accuracy: 0.968\n"
     ]
    }
   ],
   "source": [
    "#######tanh SECTION ############\n",
    "tanhParams = {'W1': np.random.randn(n_h, n_x)* np.sqrt(1. / n_x),\n",
    "                 'b1': np.zeros((n_h, 1)),\n",
    "                 'W2': np.random.randn(10, n_h)* np.sqrt(1. / n_h),\n",
    "                 'b2': np.zeros((10, 1))\n",
    "                 }\n",
    "\n",
    "start = datetime.now()\n",
    "for i in range(epoch):\n",
    "    #shuffle batch index\n",
    "    idx = np.random.permutation(X_train.shape[1])[:m]\n",
    "    X=X_train[:,idx]\n",
    "    y=y_train[:,idx]\n",
    "    #forward pass\n",
    "    forwardPass = forward(X,tanhParams,tanh)\n",
    "    #cost\n",
    "    cost = crossEntropyR2(y, forwardPass['A2'], lamda, tanhParams)\n",
    "    #back Prop\n",
    "    gradient = back(X, y, forwardPass, tanhParams,dTanh)\n",
    "    #updating weights\n",
    "    tanhParams=updater(tanhParams,gradient,eta,lamda,m)\n",
    "difference = datetime.now() - start\n",
    "print(\"Final cost:\", cost)\n",
    "print('time to train:', difference)\n",
    "\n",
    "y_hat = classifer(X_test, tanhParams, tanh)\n",
    "\n",
    "\n",
    "print('Accuracy:',sum(y_hat==y_test)*1/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
